{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35cb0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchmetrics import Accuracy, Recall, Precision, Specificity, ConfusionMatrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score,roc_curve, auc, precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from tabulate import tabulate\n",
    "from ray import tune\n",
    "\n",
    "from fractions import Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57cd9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensors to hold input and outputs.\n",
    "#第一组参数\n",
    "# beta_1=np.append(np.array([4,4,-8,-4,6,8,-4,4,-6,-8]),np.zeros(90))\n",
    "# beta_2=np.append(np.array([-2,2,4,-2,3,-4,2,-2,-3,4]),np.zeros(90))\n",
    "# beta_3=np.append(np.array([-3,-3,6,-3,-4.5,-6,3,-3,4.5,6]),np.zeros(90))\n",
    "\n",
    "#第二组参数\n",
    "\n",
    "beta_1=np.append(np.array([4,4,-8,-4,6,-4,8,4,-6,-8]),np.zeros(90))\n",
    "beta_2=np.append(np.array([-2,2,4,-2,3,2,-4,-2,-3,4]),np.zeros(90))\n",
    "beta_3=np.append(np.array([-3,-3,6,-3,-4.5,3,-6,-3,4.5,6]),np.zeros(90))\n",
    "\n",
    "\n",
    "beta_1=beta_1.reshape(-1,1)\n",
    "beta_2=beta_2.reshape(-1,1)\n",
    "beta_3=beta_3.reshape(-1,1)\n",
    "\n",
    "n1=200\n",
    "n2=200\n",
    "n3=200\n",
    "p=100\n",
    "# corval=0\n",
    "corval=0\n",
    "prior=7\n",
    "\n",
    "def sigmoid(X):\n",
    "    return .5 * (1 + np.tanh(.5 * X))\n",
    "\n",
    "def generate_data(corval,beta,n,p):\n",
    "    mean=np.zeros(p)\n",
    "    sigma=np.array([[corval**abs(i-j) for i in range(p)] for j in range(p)])\n",
    "    x=torch.tensor(np.random.multivariate_normal(mean=mean,cov=sigma,size=n))\n",
    "    Pi_test= sigmoid(x@beta)\n",
    "    y=np.random.binomial(1,Pi_test.ravel(),n)\n",
    "    x=x.to(torch.float32)\n",
    "    #y=torch.tensor(y,dtype=torch.double)\n",
    "    y=torch.tensor(y,dtype=torch.float)\n",
    "    #y=y.to(torch.float32)\n",
    "    return x,y\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def cubic_root(x):\n",
    "    return math.copysign(math.pow(abs(x), 1.0/3.0), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdf8807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    split_num=5\n",
    "    kf = KFold(n_splits=split_num)\n",
    "\n",
    "\n",
    "\n",
    "    sum_test_BCE_loss=0\n",
    "    for idx_train,idx_test in kf.split(inputs_1):\n",
    "        lambda1,lambda2,lr,ga= config[\"lambda1\"], config[\"lambda2\"],config[\"lr\"],config[\"ga\"]\n",
    "        model_1=MLP(seed=1)\n",
    "        model_2=MLP(seed=2)\n",
    "        model_3=MLP(seed=1)\n",
    "        max_iteration=1000\n",
    "        learning_rate=lr\n",
    "        params_to_optimize=list(model_1.parameters())+list(model_2.parameters())+list(model_3.parameters())\n",
    "        optimizer = torch.optim.SGD(params_to_optimize, lr=learning_rate)\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,400,500,600,700,800], gamma=ga)\n",
    "\n",
    "        BCE=[]\n",
    "\n",
    "        for t in range(max_iteration):\n",
    "            optimizer.zero_grad() # renew optimizer\n",
    "            out_1, layer1_out_1, layer2_out_1= model_1(inputs_1[idx_train])\n",
    "            out_2, layer1_out_2, layer2_out_2= model_2(inputs_2[idx_train])\n",
    "            out_3, layer1_out_3, layer2_out_3= model_3(inputs_3[idx_train])# forward propagate\n",
    "\n",
    "            # extract parameters\n",
    "            #[:-1] for leaving out bias term#\n",
    "            model_1_all_linear1_params = model_1.linear1\n",
    "            model_1_all_linear2_params= torch.cat([x.view(-1) for x in model_1.linear2.parameters()][:-1])\n",
    "            model_1_all_linear3_params= torch.cat([x.view(-1) for x in model_1.linear3.parameters()][:-1])\n",
    "           \n",
    "\n",
    "            model_2_all_linear1_params = model_1.linear1\n",
    "            model_2_all_linear2_params= torch.cat([x.view(-1) for x in model_2.linear2.parameters()][:-1])\n",
    "            model_2_all_linear3_params= torch.cat([x.view(-1) for x in model_2.linear3.parameters()][:-1])\n",
    "    \n",
    "\n",
    "            model_3_all_linear1_params = model_3.linear1\n",
    "            model_3_all_linear2_params= torch.cat([x.view(-1) for x in model_3.linear2.parameters()][:-1])\n",
    "            model_3_all_linear3_params= torch.cat([x.view(-1) for x in model_3.linear3.parameters()][:-1])\n",
    "    \n",
    "\n",
    "\n",
    "            # compute loss\n",
    "            all_linear1_params=torch.cat(( model_1_all_linear1_params.reshape(1,-1),  model_2_all_linear1_params.reshape(1,-1), model_3_all_linear1_params.reshape(1,-1)),0)\n",
    "            all_linear23_params=torch.cat((model_1_all_linear2_params,model_1_all_linear3_params,\n",
    "                                           model_2_all_linear2_params,model_2_all_linear3_params,\n",
    "                                           model_3_all_linear2_params,model_3_all_linear3_params),0)\n",
    "\n",
    "            BCE_loss_1 = loss_fn(out_1, targets_1[idx_train].reshape(-1,1))\n",
    "            BCE_loss_2 = loss_fn(out_2, targets_2[idx_train].reshape(-1,1))\n",
    "            BCE_loss_3 = loss_fn(out_3, targets_3[idx_train].reshape(-1,1))\n",
    "\n",
    "#             l1_regularization = lambda1 * smooth_l1(all_linear1_params,prior)\n",
    "            l1_regularization = lambda1 * torch.sum(torch.norm(all_linear1_params[:,prior:],p=2,dim=0))\n",
    "\n",
    "            l2_regularization = lambda2* torch.sum(all_linear23_params.pow(2))\n",
    "            BCE_loss=BCE_loss_1+BCE_loss_2+BCE_loss_3\n",
    "            loss =BCE_loss+l1_regularization +l2_regularization\n",
    "\n",
    "            # record loss\n",
    "            BCE.append(BCE_loss.item())\n",
    "\n",
    "            # compute derivative\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient descent\n",
    "            optimizer.step()\n",
    "\n",
    "            # learning rate decay\n",
    "            scheduler.step()\n",
    "        \n",
    "        \n",
    "        test_out_1, test_layer1_out_1, test_layer2_out_1= model_1(train_x1[idx_test])\n",
    "        test_out_2, test_layer1_out_2, test_layer2_out_2= model_2(train_x2[idx_test])\n",
    "        test_out_3, test_layer1_out_3, test_layer2_out_3= model_3(train_x3[idx_test])\n",
    "        \n",
    "        test_BCE_loss_1 = loss_fn(test_out_1, targets_1[idx_test].reshape(-1,1))\n",
    "        test_BCE_loss_2 = loss_fn(test_out_2, targets_2[idx_test].reshape(-1,1))\n",
    "        test_BCE_loss_3 = loss_fn(test_out_3, targets_3[idx_test].reshape(-1,1))\n",
    "        \n",
    "        test_BCE_loss=test_BCE_loss_1+test_BCE_loss_2+test_BCE_loss_3\n",
    "        \n",
    "        sum_test_BCE_loss+=test_BCE_loss.item()\n",
    "\n",
    "    #print(\"loss: \",)\n",
    "    tune.report(my_test_BCE_loss=sum_test_BCE_loss/split_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b0ef358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_2(config):\n",
    "    split_num=5\n",
    "    kf = KFold(n_splits=split_num)\n",
    "    \n",
    "    \n",
    "    lambda1,lambda2,eta,lr= config[\"lambda1\"], config[\"lambda2\"],config[\"eta\"],config[\"lr\"]\n",
    "    inputs_1,inputs_2,inputs_3=train_x1,train_x2,train_x3\n",
    "    targets_1=(1-eta)*train_y1+eta*y_prior_1\n",
    "    targets_2=(1-eta)*train_y2+eta*y_prior_2\n",
    "    targets_3=(1-eta)*train_y3+eta*y_prior_3\n",
    "    \n",
    "    \n",
    "    sum_tGM=0\n",
    "    for idx_train,idx_test in kf.split(inputs_1):\n",
    "       \n",
    "        \n",
    "        model_1=MLP(seed=1)\n",
    "        model_2=MLP(seed=2)\n",
    "        model_3=MLP(seed=1)\n",
    "        max_iteration=1000\n",
    "        learning_rate=lr\n",
    "        params_to_optimize=list(model_1.parameters())+list(model_2.parameters())+list(model_3.parameters())\n",
    "        optimizer = torch.optim.SGD(params_to_optimize, lr=learning_rate)\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,400,500,600,700,800], gamma=0.3)\n",
    "\n",
    "        BCE=[]\n",
    "\n",
    "        for t in range(max_iteration):\n",
    "            optimizer.zero_grad() # renew optimizer\n",
    "            out_1, layer1_out_1, layer2_out_1,layer3_out_1= model_1(inputs_1[idx_train])\n",
    "            out_2, layer1_out_2, layer2_out_2,layer3_out_2= model_2(inputs_2[idx_train])\n",
    "            out_3, layer1_out_3, layer2_out_3,layer3_out_3= model_3(inputs_3[idx_train])# forward propagate\n",
    "\n",
    "            # extract parameters\n",
    "            #[:-1] for leaving out bias term#\n",
    "            model_1_all_linear1_params = model_1.linear1\n",
    "            model_1_all_linear2_params= torch.cat([x.view(-1) for x in model_1.linear2.parameters()][:-1])\n",
    "            model_1_all_linear3_params= torch.cat([x.view(-1) for x in model_1.linear3.parameters()][:-1])\n",
    "            model_1_all_linear4_params= torch.cat([x.view(-1) for x in model_1.linear4.parameters()][:-1])\n",
    "           \n",
    "\n",
    "            model_2_all_linear1_params = model_1.linear1\n",
    "            model_2_all_linear2_params= torch.cat([x.view(-1) for x in model_2.linear2.parameters()][:-1])\n",
    "            model_2_all_linear3_params= torch.cat([x.view(-1) for x in model_2.linear3.parameters()][:-1])\n",
    "            model_2_all_linear4_params= torch.cat([x.view(-1) for x in model_2.linear4.parameters()][:-1])\n",
    "    \n",
    "\n",
    "            model_3_all_linear1_params = model_3.linear1\n",
    "            model_3_all_linear2_params= torch.cat([x.view(-1) for x in model_3.linear2.parameters()][:-1])\n",
    "            model_3_all_linear3_params= torch.cat([x.view(-1) for x in model_3.linear3.parameters()][:-1])\n",
    "            model_3_all_linear4_params= torch.cat([x.view(-1) for x in model_3.linear4.parameters()][:-1])\n",
    "    \n",
    "\n",
    "\n",
    "            # compute loss\n",
    "            all_linear1_params=torch.cat(( model_1_all_linear1_params.reshape(1,-1),  model_2_all_linear1_params.reshape(1,-1), model_3_all_linear1_params.reshape(1,-1)),0)\n",
    "            all_linear234_params=torch.cat((model_1_all_linear2_params,model_1_all_linear3_params, model_1_all_linear4_params,\n",
    "                                             model_2_all_linear2_params,model_2_all_linear3_params, model_2_all_linear4_params,\n",
    "                                             model_3_all_linear2_params,model_3_all_linear3_params,model_3_all_linear4_params),0)\n",
    "\n",
    "            BCE_loss_1 = loss_fn(out_1, targets_1[idx_train].reshape(-1,1))\n",
    "            BCE_loss_2 = loss_fn(out_2, targets_2[idx_train].reshape(-1,1))\n",
    "            BCE_loss_3 = loss_fn(out_3, targets_3[idx_train].reshape(-1,1))\n",
    "\n",
    "#             l1_regularization = lambda1 * smooth_l1(all_linear1_params,prior)\n",
    "            l1_regularization = lambda1 * torch.sum(torch.norm(all_linear1_params[:,prior:],p=2,dim=0))\n",
    "\n",
    "            l2_regularization = lambda2* torch.sum(all_linear234_params.pow(2))\n",
    "            BCE_loss=BCE_loss_1+BCE_loss_2+BCE_loss_3\n",
    "            loss =BCE_loss+l1_regularization +l2_regularization\n",
    "\n",
    "            # record loss\n",
    "            BCE.append(BCE_loss.item())\n",
    "\n",
    "            # compute derivative\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient descent\n",
    "            optimizer.step()\n",
    "\n",
    "            # learning rate decay\n",
    "            scheduler.step()\n",
    "        \n",
    "        \n",
    "        test_out_1, test_layer1_out_1, test_layer2_out_1,test_layer3_out_1= model_1(train_x1[idx_test])\n",
    "        test_out_2, test_layer1_out_2, test_layer2_out_2,test_layer3_out_2= model_2(train_x2[idx_test])\n",
    "        test_out_3, test_layer1_out_3, test_layer2_out_3,test_layer3_out_3= model_3(train_x3[idx_test])\n",
    "        \n",
    "\n",
    "        test_prediction_1= (model_1(train_x1[idx_test])[0]>0.5).clone().int()\n",
    "        test_target_1= train_y1[idx_test].reshape(-1,1).int()\n",
    "\n",
    "        test_prediction_2= (model_2(train_x2[idx_test])[0]>0.5).clone().int()\n",
    "        test_target_2= train_y2[idx_test].reshape(-1,1).int()\n",
    "\n",
    "        test_prediction_3= (model_3(train_x3[idx_test])[0]>0.5).clone().int()\n",
    "        test_target_3= train_y3[idx_test].reshape(-1,1).int()\n",
    "\n",
    "        test_prediction=np.append(test_prediction_1,test_prediction_2)\n",
    "        test_prediction=np.append(test_prediction,test_prediction_3)\n",
    "        test_prediction=test_prediction.tolist()\n",
    "\n",
    "        test_target=np.append(test_target_1,test_target_2)\n",
    "        test_target=np.append(test_target,test_target_3)\n",
    "        test_target=test_target.tolist()\n",
    "\n",
    "        final_matrix=confusion_matrix(test_target,test_prediction)\n",
    "        tTPR=recall_score(test_target,test_target)#TPR\n",
    "        tTNR=final_matrix[0,0]/(sum(final_matrix[0,:]))#TNR\n",
    "        tGM=math.sqrt(tTPR*tTNR)\n",
    "\n",
    "        sum_tGM+=tGM\n",
    "\n",
    "    tune.report(my_test_sum_tGM=sum_tGM/split_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "606b4828",
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR=[]\n",
    "TNR=[]\n",
    "GM=[]\n",
    "accuracy=[]\n",
    "precision=[]\n",
    "f1=[]=[]\n",
    "vsSEN=[]\n",
    "vsSPE=[]\n",
    "vsGM=[]\n",
    "vsMR=[]\n",
    "vsCCR=[]\n",
    "\n",
    "\n",
    "vs = np.zeros((1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da509cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-11-14 15:55:24 (running for 00:12:52.35)<br>Memory usage on this node: 7.8/1133.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/32 CPUs, 0/1 GPUs, 0.0/936.31 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /home/rd/ray_results/train_model_2022-11-14_15-42-31<br>Number of trials: 30/30 (30 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  ga</th><th style=\"text-align: right;\">  lambda1</th><th style=\"text-align: right;\">  lambda2</th><th style=\"text-align: right;\">  lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_e5868_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.3</td><td style=\"text-align: right;\">     0.03</td><td style=\"text-align: right;\">    0.003</td><td style=\"text-align: right;\"> 0.8</td></tr>\n",
       "<tr><td>train_model_e5868_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.5</td><td style=\"text-align: right;\">     0.03</td><td style=\"text-align: right;\">    0.001</td><td style=\"text-align: right;\"> 0.8</td></tr>\n",
       "<tr><td>train_model_e5868_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.4</td><td style=\"text-align: right;\">     0.05</td><td style=\"text-align: right;\">    0.001</td><td style=\"text-align: right;\"> 0.6</td></tr>\n",
       "<tr><td>train_model_e5868_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.4</td><td style=\"text-align: right;\">     0.05</td><td style=\"text-align: right;\">    0.002</td><td style=\"text-align: right;\"> 0.8</td></tr>\n",
       "<tr><td>train_model_e5868_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.4</td><td style=\"text-align: right;\">     0.04</td><td style=\"text-align: right;\">    0.002</td><td style=\"text-align: right;\"> 0.7</td></tr>\n",
       "<tr><td>train_model_e5868_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.4</td><td style=\"text-align: right;\">     0.03</td><td style=\"text-align: right;\">    0.002</td><td style=\"text-align: right;\"> 0.8</td></tr>\n",
       "<tr><td>train_model_e5868_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.4</td><td style=\"text-align: right;\">     0.04</td><td style=\"text-align: right;\">    0.002</td><td style=\"text-align: right;\"> 0.7</td></tr>\n",
       "<tr><td>train_model_e5868_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.4</td><td style=\"text-align: right;\">     0.03</td><td style=\"text-align: right;\">    0.001</td><td style=\"text-align: right;\"> 0.7</td></tr>\n",
       "<tr><td>train_model_e5868_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.5</td><td style=\"text-align: right;\">     0.03</td><td style=\"text-align: right;\">    0.003</td><td style=\"text-align: right;\"> 0.8</td></tr>\n",
       "<tr><td>train_model_e5868_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.5</td><td style=\"text-align: right;\">     0.03</td><td style=\"text-align: right;\">    0.001</td><td style=\"text-align: right;\"> 0.6</td></tr>\n",
       "<tr><td>train_model_e5868_00010</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.4</td><td style=\"text-align: right;\">     0.03</td><td style=\"text-align: right;\">    0.003</td><td style=\"text-align: right;\"> 0.8</td></tr>\n",
       "<tr><td>train_model_e5868_00011</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.4</td><td style=\"text-align: right;\">     0.04</td><td style=\"text-align: right;\">    0.002</td><td style=\"text-align: right;\"> 0.8</td></tr>\n",
       "<tr><td>train_model_e5868_00012</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.4</td><td style=\"text-align: right;\">     0.03</td><td style=\"text-align: right;\">    0.003</td><td style=\"text-align: right;\"> 0.8</td></tr>\n",
       "<tr><td>train_model_e5868_00013</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.4</td><td style=\"text-align: right;\">     0.03</td><td style=\"text-align: right;\">    0.002</td><td style=\"text-align: right;\"> 0.6</td></tr>\n",
       "<tr><td>train_model_e5868_00014</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.4</td><td style=\"text-align: right;\">     0.05</td><td style=\"text-align: right;\">    0.002</td><td style=\"text-align: right;\"> 0.7</td></tr>\n",
       "<tr><td>train_model_e5868_00015</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.3</td><td style=\"text-align: right;\">     0.03</td><td style=\"text-align: right;\">    0.002</td><td style=\"text-align: right;\"> 0.8</td></tr>\n",
       "<tr><td>train_model_e5868_00016</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.4</td><td style=\"text-align: right;\">     0.03</td><td style=\"text-align: right;\">    0.003</td><td style=\"text-align: right;\"> 0.7</td></tr>\n",
       "<tr><td>train_model_e5868_00017</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.4</td><td style=\"text-align: right;\">     0.04</td><td style=\"text-align: right;\">    0.001</td><td style=\"text-align: right;\"> 0.8</td></tr>\n",
       "<tr><td>train_model_e5868_00018</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.4</td><td style=\"text-align: right;\">     0.05</td><td style=\"text-align: right;\">    0.003</td><td style=\"text-align: right;\"> 0.7</td></tr>\n",
       "<tr><td>train_model_e5868_00019</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 0.3</td><td style=\"text-align: right;\">     0.05</td><td style=\"text-align: right;\">    0.003</td><td style=\"text-align: right;\"> 0.7</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 10 more trials not shown (10 PENDING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    setup_seed(i)\n",
    "    x1,y1=generate_data(corval,beta_1,n1,p)\n",
    "    x2,y2=generate_data(corval,beta_2,n2,p)\n",
    "    x3,y3=generate_data(corval,beta_3,n3,p)\n",
    "\n",
    "    train_x1, test_x1, train_y1, test_y1= train_test_split(x1,y1, test_size=0.2, random_state=1)\n",
    "    train_x2, test_x2, train_y2, test_y2= train_test_split(x2,y2, test_size=0.2, random_state=1)\n",
    "    train_x3, test_x3, train_y3, test_y3= train_test_split(x3,y3, test_size=0.2, random_state=1)\n",
    "    \n",
    "     # switch training set\n",
    "    inputs_1,targets_1 = train_x1,train_y1\n",
    "    inputs_2,targets_2 = train_x2,train_y2\n",
    "    inputs_3,targets_3 = train_x3,train_y3\n",
    "\n",
    "    # switching testing set\n",
    "    test_inputs_1, test_targets_1 = test_x1,test_y1\n",
    "    test_inputs_2, test_targets_2 = test_x2,test_y2\n",
    "    test_inputs_3, test_targets_3 = test_x3,test_y3\n",
    "    \n",
    "################## first step######################################  \n",
    "    prior=7\n",
    "# choose lambda1,lambda2 by ray tune\n",
    "    class MLP(torch.nn.Module):\n",
    "        def __init__(self,seed):\n",
    "            super(MLP, self).__init__()\n",
    "            torch.manual_seed(seed)\n",
    "            self.linear1 = torch.nn.Parameter(torch.randn(p))\n",
    "            self.linear2 = torch.nn.Linear(p,10)\n",
    "            self.linear3 = torch.nn.Linear(10,1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            layer1_out = self.linear1*x\n",
    "            layer2_out = F.relu(self.linear2(layer1_out))\n",
    "            out= torch.sigmoid(self.linear3(layer2_out))\n",
    "            return out, layer1_out, layer2_out\n",
    "\n",
    "\n",
    "    config={\n",
    "            \"lr\":tune.choice([0.6,0.7,0.8]),\n",
    "            \"lambda1\": tune.choice([0.03,0.04,0.05]),\n",
    "            \"lambda2\": tune.choice([0.001,0.002,0.003]),\n",
    "            \"ga\":tune.choice([0.3,0.4,0.5])\n",
    "        }\n",
    "\n",
    "    result = tune.run(\n",
    "            train_model,  \n",
    "            config=config,\n",
    "            num_samples=30)\n",
    "    \n",
    "################## second step########################################    \n",
    "\n",
    "    final_lambda=result.get_best_config('my_test_BCE_loss',mode='min')\n",
    "    lambda1,lambda2,lr,ga=final_lambda[\"lambda1\"],final_lambda[\"lambda2\"],final_lambda[\"lr\"],final_lambda[\"ga\"]\n",
    "    learning_rate=lr\n",
    "    max_iteration=1000\n",
    "    \n",
    "    BCE=[]\n",
    "    \n",
    "    model_1=MLP(seed=1)\n",
    "    model_2=MLP(seed=2)\n",
    "    model_3=MLP(seed=1)\n",
    "\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    params_to_optimize=list(model_1.parameters())+list(model_2.parameters())+list(model_3.parameters())\n",
    "    optimizer = torch.optim.SGD(params_to_optimize, lr=learning_rate)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,400,500,600,700,800], gamma=ga)\n",
    "\n",
    "    loss_record=[]\n",
    "    \n",
    "    for t in range(max_iteration):\n",
    "    \n",
    "        # renew optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward propagate\n",
    "        out_1, layer1_out_1, layer2_out_1= model_1(inputs_1)\n",
    "        out_2, layer1_out_2, layer2_out_2= model_2(inputs_2)\n",
    "        out_3, layer1_out_3, layer2_out_3= model_3(inputs_3)\n",
    "\n",
    "\n",
    "\n",
    "        # extract parameters\n",
    "        #[:-1] for leaving out bias term#\n",
    "        model_1_all_linear1_params = model_1.linear1\n",
    "        model_1_all_linear2_params= torch.cat([x.view(-1) for x in model_1.linear2.parameters()][:-1])\n",
    "        model_1_all_linear3_params= torch.cat([x.view(-1) for x in model_1.linear3.parameters()][:-1])\n",
    "\n",
    "\n",
    "        model_2_all_linear1_params = model_1.linear1\n",
    "        model_2_all_linear2_params= torch.cat([x.view(-1) for x in model_2.linear2.parameters()][:-1])\n",
    "        model_2_all_linear3_params= torch.cat([x.view(-1) for x in model_2.linear3.parameters()][:-1])\n",
    "\n",
    "\n",
    "        model_3_all_linear1_params = model_3.linear1\n",
    "        model_3_all_linear2_params= torch.cat([x.view(-1) for x in model_3.linear2.parameters()][:-1])\n",
    "        model_3_all_linear3_params= torch.cat([x.view(-1) for x in model_3.linear3.parameters()][:-1])\n",
    "\n",
    "\n",
    "        # compute loss\n",
    "\n",
    "        all_linear1_params=torch.cat(( model_1_all_linear1_params.reshape(1,-1),  model_2_all_linear1_params.reshape(1,-1), model_3_all_linear1_params.reshape(1,-1)),0)\n",
    "        all_linear23_params=torch.cat((model_1_all_linear2_params,model_1_all_linear3_params, \n",
    "                                         model_2_all_linear2_params,model_2_all_linear3_params,\n",
    "                                         model_3_all_linear2_params,model_3_all_linear3_params),0)\n",
    "\n",
    "        BCE_loss_1 = loss_fn(out_1, targets_1.reshape(-1,1))\n",
    "        BCE_loss_2 = loss_fn(out_2, targets_2.reshape(-1,1))\n",
    "        BCE_loss_3 = loss_fn(out_3, targets_3.reshape(-1,1))\n",
    "\n",
    "    #     l1_regularization = lambda1 * smooth_l1(all_linear1_params,prior)\n",
    "\n",
    "        l1_regularization = lambda1 * torch.sum(torch.norm(all_linear1_params[:,prior:],p=2,dim=0))\n",
    "\n",
    "        l2_regularization = lambda2 * torch.sum(all_linear23_params.pow(2))\n",
    "        BCE_loss=BCE_loss_1+BCE_loss_2+BCE_loss_3\n",
    "        loss =BCE_loss+l1_regularization +l2_regularization\n",
    "\n",
    "        loss_record.append(loss.item())\n",
    "        # record loss\n",
    "        BCE.append(BCE_loss.item())\n",
    "\n",
    "        # compute derivative\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        # learning rate decay\n",
    "        scheduler.step()\n",
    "    \n",
    "################## Y_prior##################    \n",
    "    y_prior_1= (model_1(train_x1)[0]).detach().squeeze()\n",
    "\n",
    "    y_prior_2= (model_2(train_x2)[0]).detach().squeeze()\n",
    "\n",
    "    y_prior_3= (model_3(train_x3)[0]).detach().squeeze()\n",
    "    \n",
    "    \n",
    "################## third step##################\n",
    "\n",
    "    #no prior\n",
    "    prior=0\n",
    "    # choose lambda1,lambda2 by ray tune\n",
    "    class MLP(torch.nn.Module):\n",
    "        def __init__(self,seed):\n",
    "            super(MLP, self).__init__()\n",
    "            torch.manual_seed(seed)\n",
    "            self.linear1 = torch.nn.Parameter(torch.randn(p))\n",
    "            self.linear2 = torch.nn.Linear(p,10)\n",
    "            self.linear3 = torch.nn.Linear(10,10)\n",
    "            self.linear4 = torch.nn.Linear(10,1)\n",
    "        def forward(self, x):\n",
    "            layer1_out = self.linear1*x\n",
    "            layer2_out = F.relu(self.linear2(layer1_out))\n",
    "            layer3_out = F.relu(self.linear3(layer2_out))\n",
    "            out= torch.sigmoid(self.linear4(layer3_out))\n",
    "            return out, layer1_out, layer2_out,layer3_out\n",
    "\n",
    "    config={\n",
    "            \"lambda1\": tune.choice([0.03,0.04,0.05]),\n",
    "            \"lambda2\": tune.choice([0.001,0.002,0.003]),\n",
    "             \"lr\": tune.choice([0.7,0.8,0.9]),\n",
    "            \"eta\": tune.choice([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1])\n",
    "        }\n",
    "\n",
    "    result = tune.run(\n",
    "            train_model_2,  \n",
    "            config=config,\n",
    "            num_samples=270)\n",
    "    \n",
    "\n",
    "################## fourth step##################\n",
    "\n",
    "    #no prior\n",
    "    prior=0\n",
    "    #0.4,0.4\n",
    "    # set hyperparamters\n",
    "    final_parameter=result.get_best_config('my_test_sum_tGM',mode='max')\n",
    "    lambda1,lambda2,eta,lr=final_parameter[\"lambda1\"],final_parameter[\"lambda2\"],final_parameter[\"eta\"],final_parameter[\"lr\"]\n",
    "    learning_rate=lr\n",
    "    max_iteration=1000\n",
    "\n",
    "    inputs_1,inputs_2,inputs_3=train_x1,train_x2,train_x3\n",
    "    targets_1=(1-eta)*train_y1+eta*y_prior_1\n",
    "    targets_2=(1-eta)*train_y2+eta*y_prior_2\n",
    "    targets_3=(1-eta)*train_y3+eta*y_prior_3\n",
    "\n",
    "    # switching testing set\n",
    "    test_inputs_1, test_targets_1 = test_x1,test_y1\n",
    "    test_inputs_2, test_targets_2 = test_x2,test_y2\n",
    "    test_inputs_3, test_targets_3 = test_x3,test_y3\n",
    "\n",
    "    # record loss descent\n",
    "    BCE=[]\n",
    "\n",
    "\n",
    "    # main nn object\n",
    "    class MLP(torch.nn.Module):\n",
    "        def __init__(self,seed):\n",
    "            super(MLP, self).__init__()\n",
    "            torch.manual_seed(seed)\n",
    "            self.linear1 = torch.nn.Parameter(torch.randn(p))\n",
    "            self.linear2 = torch.nn.Linear(p,10)\n",
    "            self.linear3 = torch.nn.Linear(10,10)\n",
    "            self.linear4 = torch.nn.Linear(10,1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            layer1_out = self.linear1*x\n",
    "            layer2_out = F.relu(self.linear2(layer1_out))\n",
    "            layer3_out = F.relu(self.linear3(layer2_out))\n",
    "            out= torch.sigmoid(self.linear4(layer3_out))\n",
    "            return out, layer1_out, layer2_out,layer3_out\n",
    "\n",
    "\n",
    "\n",
    "    model_1=MLP(seed=1)\n",
    "    model_2=MLP(seed=2)\n",
    "    model_3=MLP(seed=1)\n",
    "\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    params_to_optimize=list(model_1.parameters())+list(model_2.parameters())+list(model_3.parameters())\n",
    "    optimizer = torch.optim.SGD(params_to_optimize, lr=learning_rate)\n",
    "\n",
    "    # learning rate decay scheme\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,400,500,600,700,800], gamma=0.3)\n",
    "\n",
    "    loss_record=[]\n",
    "    # loop for max_iteration times\n",
    "    for t in range(max_iteration):\n",
    "\n",
    "        # renew optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward propagate\n",
    "        out_1, layer1_out_1, layer2_out_1,layer3_out_1= model_1(inputs_1)\n",
    "        out_2, layer1_out_2, layer2_out_2,layer3_out_2= model_2(inputs_2)\n",
    "        out_3, layer1_out_3, layer2_out_3,layer3_out_3= model_3(inputs_3)\n",
    "\n",
    "\n",
    "\n",
    "        # extract parameters\n",
    "        #[:-1] for leaving out bias term#\n",
    "        model_1_all_linear1_params = model_1.linear1\n",
    "        model_1_all_linear2_params= torch.cat([x.view(-1) for x in model_1.linear2.parameters()][:-1])\n",
    "        model_1_all_linear3_params= torch.cat([x.view(-1) for x in model_1.linear3.parameters()][:-1])\n",
    "        model_1_all_linear4_params= torch.cat([x.view(-1) for x in model_1.linear4.parameters()][:-1])\n",
    "\n",
    "        model_2_all_linear1_params = model_1.linear1\n",
    "        model_2_all_linear2_params= torch.cat([x.view(-1) for x in model_2.linear2.parameters()][:-1])\n",
    "        model_2_all_linear3_params= torch.cat([x.view(-1) for x in model_2.linear3.parameters()][:-1])\n",
    "        model_2_all_linear4_params= torch.cat([x.view(-1) for x in model_2.linear4.parameters()][:-1])\n",
    "\n",
    "        model_3_all_linear1_params = model_3.linear1\n",
    "        model_3_all_linear2_params= torch.cat([x.view(-1) for x in model_3.linear2.parameters()][:-1])\n",
    "        model_3_all_linear3_params= torch.cat([x.view(-1) for x in model_3.linear3.parameters()][:-1])\n",
    "        model_3_all_linear4_params= torch.cat([x.view(-1) for x in model_3.linear4.parameters()][:-1])\n",
    "\n",
    "\n",
    "        # compute loss\n",
    "\n",
    "        all_linear1_params=torch.cat(( model_1_all_linear1_params.reshape(1,-1),  model_2_all_linear1_params.reshape(1,-1), model_3_all_linear1_params.reshape(1,-1)),0)\n",
    "        all_linear234_params=torch.cat((model_1_all_linear2_params,model_1_all_linear3_params, model_1_all_linear4_params,\n",
    "                                         model_2_all_linear2_params,model_2_all_linear3_params, model_2_all_linear4_params,\n",
    "                                         model_3_all_linear2_params,model_3_all_linear3_params,model_3_all_linear4_params),0)\n",
    "\n",
    "        BCE_loss_1 = loss_fn(out_1, targets_1.reshape(-1,1))\n",
    "        BCE_loss_2 = loss_fn(out_2, targets_2.reshape(-1,1))\n",
    "        BCE_loss_3 = loss_fn(out_3, targets_3.reshape(-1,1))\n",
    "\n",
    "    #     l1_regularization = lambda1 * smooth_l1(all_linear1_params,prior)\n",
    "\n",
    "        l1_regularization = lambda1 * torch.sum(torch.norm(all_linear1_params[:,prior:],p=2,dim=0))\n",
    "\n",
    "        l2_regularization = lambda2 * torch.sum(all_linear234_params.pow(2))\n",
    "        BCE_loss=BCE_loss_1+BCE_loss_2+BCE_loss_3\n",
    "        loss =BCE_loss+l1_regularization +l2_regularization\n",
    "\n",
    "        loss_record.append(loss.item())\n",
    "        # record loss\n",
    "        BCE.append(BCE_loss.item())\n",
    "\n",
    "        # compute derivative\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        # learning rate decay\n",
    "        scheduler.step()\n",
    "    \n",
    "####################################   prediction ####################################      \n",
    "\n",
    "    prediction_1= (model_1(test_x1)[0]>0.5).clone().int()\n",
    "    target_1= test_y1.reshape(-1,1).int()\n",
    "\n",
    "    prediction_2= (model_2(test_x2)[0]>0.5).clone().int()\n",
    "    target_2= test_y2.reshape(-1,1).int()\n",
    "\n",
    "    prediction_3= (model_3(test_x3)[0]>0.5).clone().int()\n",
    "    target_3= test_y3.reshape(-1,1).int()\n",
    "\n",
    "    prediction=np.append(prediction_1,prediction_2)\n",
    "    prediction=np.append(prediction,prediction_3)\n",
    "    prediction=prediction.tolist()\n",
    "\n",
    "    target=np.append(target_1,target_2)\n",
    "    target=np.append(target,target_3)\n",
    "    target=target.tolist()\n",
    "\n",
    "    final_matrix=confusion_matrix(target,prediction)\n",
    "    TPR.append(recall_score(target,prediction))#TPR\n",
    "    TNR.append(final_matrix[0,0]/(sum(final_matrix[0,:])))#TNR\n",
    "    GM.append(math.sqrt(TPR[-1]*TNR[-1]))\n",
    "    accuracy.append(accuracy_score(target,prediction))\n",
    "    precision.append(precision_score(target,prediction))\n",
    "    f1.append(f1_score(target,prediction))\n",
    "    \n",
    "    \n",
    "####################################   variable selection ####################################     \n",
    "    model_1_weight=model_1_all_linear1_params.detach().numpy().copy().reshape(1,-1)\n",
    "    model_2_weight=model_2_all_linear1_params.detach().numpy().copy().reshape(1,-1)\n",
    "    model_3_weight=model_3_all_linear1_params.detach().numpy().copy().reshape(1,-1)\n",
    "\n",
    "    threshold=0.12\n",
    "    mcl_w=np.zeros(p)\n",
    "\n",
    "    for j in range(p):\n",
    "        mcl_w[j]=cubic_root(model_1_weight[0,j]*model_2_weight[0,j]*model_3_weight[0,j])\n",
    "\n",
    "    max_mcl_w=max(abs(mcl_w))\n",
    "    for j in range(p):\n",
    "        if (abs(mcl_w[j]))<=threshold*max_mcl_w:\n",
    "            model_1_weight[0,j]=0\n",
    "            model_2_weight[0,j]=0\n",
    "            model_3_weight[0,j]=0\n",
    "        else:\n",
    "            model_1_weight[0,j]=model_1_weight[0,j]\n",
    "            model_2_weight[0,j]=model_2_weight[0,j]\n",
    "            model_3_weight[0,j]=model_3_weight[0,j]\n",
    "    \n",
    "    index=np.nonzero(model_1_weight)[1]\n",
    "    \n",
    "    vs[:,index]+=1\n",
    "    \n",
    "    beta=np.array(beta_1)\n",
    "    trueindex=np.where(beta!=0)[0]\n",
    "    trueindex=trueindex.tolist()\n",
    "    trueindex=set(trueindex)\n",
    "    \n",
    "    index=np.nonzero(model_1_weight)[1]\n",
    "    index=index.tolist()\n",
    "    index=set(index)\n",
    "    \n",
    "    TP=len(trueindex.intersection(index))\n",
    "    FP=len(index.difference(trueindex))\n",
    "    FN=len(trueindex)-TP\n",
    "    TN=p-len(trueindex)-FP\n",
    "    vsSEN.append(TP/(TP+FN))\n",
    "    vsSPE.append(TN/(TN+FP))\n",
    "    vsGM.append(math.sqrt(vsSEN[-1]*vsSPE[-1]))\n",
    "    vsMR.append((FP+FN)/(TP+FN+TN+FP))\n",
    "    vsCCR.append(1-vsMR[-1])\n",
    "    \n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edda6189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8777426666900352"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8656632371183454"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8712318648605114"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8716666666666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8708370617308188"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.873486332881226"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################   prediction ####################################   \n",
    "from numpy import *\n",
    "mean(TPR)\n",
    "mean(TNR)\n",
    "mean(GM)\n",
    "mean(accuracy)\n",
    "mean(precision)\n",
    "mean(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "893d5e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc211dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9911111111111112"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9903957234204619"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.009000000000000001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.991"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################   variable selection ####################################   \n",
    "mean(vsSEN)\n",
    "mean(vsSPE)\n",
    "mean(vsGM)\n",
    "mean(vsMR)\n",
    "mean(vsCCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8234796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f5686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
